# Contexto Final: Pipeline de Jobs y Eliminaci√≥n de Duplicaci√≥n en Processor
**Fecha**: 2025-10-05
**Estado**: ‚úÖ Completado - Sistema optimizado y sin duplicaci√≥n

---

## üéØ Resumen Ejecutivo

Se complet√≥ la optimizaci√≥n del pipeline de carga de datos para comunicados de recaudo, eliminando la duplicaci√≥n de funcionalidad entre Jobs y Processor Steps, y estableciendo un flujo secuencial claro:

**Jobs ‚Üí Carga de Datos (CSV + Excel)**
**Processor ‚Üí Solo Transformaciones SQL**

---

## üìä Arquitectura Final del Sistema

### Flujo Secuencial Completo

```
1. ProcessCollectionRunValidation
   ‚îî‚îÄ Valida archivos subidos
   ‚îî‚îÄ Dispara chain de jobs secuencial

2. LoadCsvDataSourcesJob - BASCAR (Queue: default, Timeout: 4h, Tries: 1)
   ‚îî‚îÄ Carga solo base-cartera.csv
   ‚îî‚îÄ Usa: ResilientCsvImporter (l√≠nea por l√≠nea, UTF-8 conversion)
   ‚îî‚îÄ Performance: ~26 min para 255k registros

3. LoadCsvDataSourcesJob - BAPRPO (Queue: default, Timeout: 4h, Tries: 1)
   ‚îî‚îÄ Carga solo base-produccion-por-poliza.csv
   ‚îî‚îÄ Usa: ResilientCsvImporter (l√≠nea por l√≠nea, UTF-8 conversion)
   ‚îî‚îÄ Performance: ~2-3 min para 50k registros

4. LoadCsvDataSourcesJob - DATPOL (Queue: default, Timeout: 4h, Tries: 1)
   ‚îî‚îÄ Carga solo datpol.csv
   ‚îî‚îÄ Usa: ResilientCsvImporter (l√≠nea por l√≠nea, UTF-8 conversion)
   ‚îî‚îÄ Performance: ~5-7 min para 68k registros

5. LoadExcelWithCopyJob - DETTRA (Queue: default, Timeout: 60 min, Tries: 1)
   ‚îî‚îÄ Convierte detalle-trabajadores.xlsx ‚Üí CSV con Go streaming
   ‚îî‚îÄ Carga CSV con PostgreSQL COPY

6. LoadExcelWithCopyJob - PAGAPL (Queue: default, Timeout: 60 min, Tries: 1)
   ‚îî‚îÄ Convierte pagos-aplicados.xlsx ‚Üí CSV con Go streaming
   ‚îî‚îÄ Carga CSV con PostgreSQL COPY

7. LoadExcelWithCopyJob - PAGPLA (Queue: default, Timeout: 60 min, Tries: 1)
   ‚îî‚îÄ Convierte pagos-planilla.xlsx ‚Üí CSV con Go streaming
   ‚îî‚îÄ Carga CSV con PostgreSQL COPY

8. ProcessCollectionDataJob (Queue: default, Timeout: 30 min, Tries: 3)
   ‚îî‚îÄ Ejecuta ConstitucionMoraAportantesProcessor
       ‚îî‚îÄ Paso 1: ValidateDataIntegrityStep (valida que jobs 2-7 cargaron datos)
       ‚îî‚îÄ Paso 2+: Transformaciones SQL (filtros, cruces, generaci√≥n archivos)
```

---

## üîß Cambios Implementados en Esta Sesi√≥n

### 1. Refactorizaci√≥n de LoadCsvDataSourcesJob (Patr√≥n Consistente)

**Problema Identificado:**
- `LoadCsvDataSourcesJob` procesaba TODOS los CSV en un solo job (inconsistente)
- `LoadExcelWithCopyJob` procesaba UN archivo por job (correcto)
- Faltaba consistencia y reutilizaci√≥n

**Soluci√≥n Implementada:**

#### `app/Jobs/LoadCsvDataSourcesJob.php` - REFACTORIZADO
```php
// ANTES: Procesaba todos los CSV de un run
public function __construct(private readonly int $runId)

// DESPU√âS: Procesa UN archivo CSV espec√≠fico
public function __construct(
    private readonly int $fileId,
    private readonly string $dataSourceCode
)
```

**Cambios clave:**
- Constructor ahora recibe `$fileId` y `$dataSourceCode` (igual que LoadExcelWithCopyJob)
- Carga solo el archivo con ese ID: `CollectionNoticeRunFile::find($this->fileId)`
- Limpia solo la tabla correspondiente al dataSourceCode
- Logs mejorados con formato visual consistente

#### `app/Jobs/ProcessCollectionRunValidation.php` - ACTUALIZADO
```php
// ANTES: Un job para todos los CSV
$chain = [new LoadCsvDataSourcesJob($run->id)];

// DESPU√âS: Un job por cada archivo CSV
foreach ($csvFiles as $file) {
    $chain[] = new LoadCsvDataSourcesJob($file->id, $file->dataSource->code);
}
```

**Ventajas de la refactorizaci√≥n:**
- ‚úÖ **Consistencia**: Mismo patr√≥n para CSV y Excel
- ‚úÖ **Reutilizaci√≥n**: Sirve para cualquier tipo de comunicado (no hardcoded a BASCAR/BAPRPO/DATPOL)
- ‚úÖ **Granularidad**: Logs y errores espec√≠ficos por archivo
- ‚úÖ **Escalabilidad**: F√°cil agregar nuevos data sources CSV sin modificar el job
- ‚úÖ **Paralelizaci√≥n futura**: Si se necesita, f√°cil convertir a batch

**Chain resultante:**
```php
$chain = [
    new LoadCsvDataSourcesJob(file_id: 1, 'BASCAR'),
    new LoadCsvDataSourcesJob(file_id: 2, 'BAPRPO'),
    new LoadCsvDataSourcesJob(file_id: 3, 'DATPOL'),
    new LoadExcelWithCopyJob(file_id: 4, 'DETTRA'),
    new LoadExcelWithCopyJob(file_id: 5, 'PAGAPL'),
    new LoadExcelWithCopyJob(file_id: 6, 'PAGPLA'),
    new ProcessCollectionDataJob(run_id: 1),
];
```

### 2. Eliminaci√≥n de Duplicaci√≥n en Processor

**Problema Identificado:**
- Los primeros 3 steps del processor duplicaban EXACTAMENTE lo que ya hac√≠an los jobs:
  - `LoadCsvDataSourcesStep` ‚ùå (duplicaba `LoadCsvDataSourcesJob`)
  - `ConvertExcelToCSVStep` ‚ùå (duplicaba `LoadExcelWithCopyJob` - conversi√≥n)
  - `LoadExcelCSVsStep` ‚ùå (duplicaba `LoadExcelWithCopyJob` - carga)

**Archivos Modificados:**

#### `app/UseCases/Recaudo/Comunicados/Processors/ConstitucionMoraAportantesProcessor.php`
```php
// ANTES (l√≠neas 44-57): 10 dependencias inyectadas
public function __construct(
    DataSourceTableManager $tableManager,
    FilesystemFactory $filesystem,
    private readonly LoadCsvDataSourcesStep $loadCsvDataSourcesStep,      // ‚ùå ELIMINADO
    private readonly ConvertExcelToCSVStep $convertExcelToCSVStep,        // ‚ùå ELIMINADO
    private readonly LoadExcelCSVsStep $loadExcelCSVsStep,                // ‚ùå ELIMINADO
    private readonly ValidateDataIntegrityStep $validateDataStep,
    private readonly FilterBascarByPeriodStep $filterBascarStep,
    // ... resto
)

// DESPU√âS (l√≠neas 44-54): 7 dependencias inyectadas
public function __construct(
    DataSourceTableManager $tableManager,
    FilesystemFactory $filesystem,
    private readonly ValidateDataIntegrityStep $validateDataStep,         // ‚úÖ √öNICO STEP DE VALIDACI√ìN
    private readonly FilterBascarByPeriodStep $filterBascarStep,
    // ... resto de steps SQL
)
```

```php
// ANTES (l√≠neas 114-153): 11 steps (3 de carga + 1 validaci√≥n + 7 SQL)
protected function defineSteps(): array
{
    return [
        $this->loadCsvDataSourcesStep,        // ‚ùå ELIMINADO
        $this->convertExcelToCSVStep,         // ‚ùå ELIMINADO
        $this->loadExcelCSVsStep,             // ‚ùå ELIMINADO
        $this->validateDataStep,
        $this->generateBascarKeysStep,
        // ...
    ];
}

// DESPU√âS (l√≠neas 104-137): 8 steps (1 validaci√≥n + 7 SQL)
protected function defineSteps(): array
{
    return [
        // === FASE 1: VALIDACI√ìN DE DATOS CARGADOS ===
        // Verifica que los jobs previos cargaron correctamente:
        // - BASCAR, BAPRPO, DATPOL (LoadCsvDataSourcesJob)
        // - DETTRA, PAGAPL, PAGPLA (LoadExcelWithCopyJob)
        $this->validateDataStep,              // ‚úÖ √öNICO STEP DE VALIDACI√ìN

        // === FASE 2: TRANSFORMACI√ìN Y CRUCE DE DATOS SQL ===
        $this->generateBascarKeysStep,
        $this->generatePagaplKeysStep,
        // ... resto de steps SQL
    ];
}
```

### 2. Implementaci√≥n de Validaci√≥n de Datos Cargados por Jobs

**Archivo:** `app/UseCases/Recaudo/Comunicados/Steps/ValidateDataIntegrityStep.php`

**Funcionalidad:**
- ‚úÖ Valida que los 6 data sources tengan registros en BD para el `run_id`
- ‚úÖ Reporta conteos por tabla
- ‚úÖ Falla si alg√∫n data source tiene 0 registros (indica que los jobs fallaron)
- ‚úÖ Logs detallados con emojis para f√°cil identificaci√≥n

```php
private const TABLE_MAP = [
    'BASCAR' => 'data_source_bascar',
    'BAPRPO' => 'data_source_baprpo',
    'DATPOL' => 'data_source_datpol',
    'DETTRA' => 'data_source_dettra',
    'PAGAPL' => 'data_source_pagapl',
    'PAGPLA' => 'data_source_pagpla',
];

public function execute(ProcessingContextDto $context): ProcessingContextDto
{
    // Contar registros para este run_id en cada tabla
    foreach ($expectedDataSources as $dataSourceCode) {
        $recordCount = DB::table($tableName)
            ->where('run_id', $run->id)
            ->count();

        if ($recordCount === 0) {
            // FALLA: Job de carga no funcion√≥
            return $context->addError(...);
        }
    }

    // Reporta estad√≠sticas completas
    Log::info('‚úÖ Validaci√≥n de integridad completada', [
        'data_sources_validated' => count($validationResults),
        'total_records_loaded' => number_format($totalRecords),
        'validation_results' => $validationResults,
    ]);
}
```

---

## üìÅ Estado de Archivos Clave

### Jobs (app/Jobs/)

#### `ProcessCollectionRunValidation.php`
- **Queue**: `default`
- **Timeout**: 900s (15 min)
- **Tries**: 3
- **Funci√≥n**: Valida archivos y dispara chain secuencial
- **L√≠nea 114-151**: Implementaci√≥n de `Bus::chain()` para ejecuci√≥n secuencial

#### `LoadCsvDataSourcesJob.php`
- **Queue**: `default`
- **Timeout**: 14400s (4 horas)
- **Tries**: 1
- **Funci√≥n**: Carga CSV (BASCAR, BAPRPO, DATPOL) con ResilientCsvImporter
- **L√≠neas 91-106**: Idempotencia (limpia tablas antes de insertar)
- **Servicio**: `ResilientCsvImporter` con UTF-8 conversion autom√°tica

#### `LoadExcelWithCopyJob.php`
- **Queue**: `default`
- **Timeout**: 3600s (60 min)
- **Tries**: 1
- **Funci√≥n**: Convierte Excel‚ÜíCSV con Go + carga con PostgreSQL COPY
- **L√≠neas 98-111**: Idempotencia (limpia tablas antes de insertar)

#### `ProcessCollectionDataJob.php`
- **Queue**: `default`
- **Timeout**: 1800s (30 min)
- **Tries**: 3
- **Funci√≥n**: Ejecuta processor (solo transformaciones SQL)
- **L√≠nea 107**: `$processor->process($run)` - ejecuta pipeline de steps

### Services (app/Services/Recaudo/)

#### `ResilientCsvImporter.php`
- **Chunk Size**: 10,000 registros
- **Caracter√≠sticas**:
  - ‚úÖ Procesa l√≠nea por l√≠nea con transacciones individuales
  - ‚úÖ Conversi√≥n autom√°tica Latin1‚ÜíUTF-8
  - ‚úÖ Log de errores en tabla `csv_import_error_logs`
  - ‚úÖ No falla todo el proceso por errores individuales
- **Performance**: ~23 min para 255k registros (BASCAR)

#### `PostgreSQLCopyImporter.php`
- **M√©todo**: Usa `psql` CLI con `COPY FROM STDIN`
- **Performance**: 10-50x m√°s r√°pido que inserts
- **Configuraci√≥n**:
  - `ESCAPE = QUOTE` para evitar problemas con backslashes
  - `NULL ''` para valores vac√≠os

### Processor y Steps (app/UseCases/Recaudo/Comunicados/)

#### `Processors/ConstitucionMoraAportantesProcessor.php`
- **Responsabilidad**: Solo transformaciones SQL (NO carga datos)
- **Steps**: 8 total (1 validaci√≥n + 7 SQL)

#### `Steps/ValidateDataIntegrityStep.php`
- **Responsabilidad**: Valida que jobs previos cargaron datos correctamente
- **Valida**: 6 tablas (BASCAR, BAPRPO, DATPOL, DETTRA, PAGAPL, PAGPLA)
- **Falla si**: Alguna tabla tiene 0 registros para el `run_id`

---

## üîç Soluciones Implementadas (Sesiones Previas)

### 1. UTF-8 Encoding (DATPOL - 28% errores ‚Üí 100% √©xito)
**Problema**: 19,375 errores por caracteres Latin1 (CASTA√ëEDA ‚Üí CASTA?EDA)
**Soluci√≥n**: `ResilientCsvImporter::ensureUtf8Encoding()` - conversi√≥n autom√°tica
**Resultado**: 68,406 registros, 0 errores

### 2. PostgreSQL COPY - psql not found
**Problema**: `sh: 1: psql: not found`
**Soluci√≥n**: Rebuild de Docker container con `postgresql-client`
**Comando**: `docker-compose build && docker-compose up -d`

### 3. Worker Timeout (90 segundos)
**Problema**: Jobs siendo killed despu√©s de 90s
**Soluci√≥n**:
- Horizon config: `timeout => 14400` (supervisor-default)
- docker-compose: `--timeout=3600`
- Jobs: `$timeout = 14400`

### 4. Ejecuci√≥n Paralela ‚Üí Secuencial
**Problema**: Jobs corriendo en paralelo causando conflictos
**Soluci√≥n**: Cambio de `Bus::batch()` a `Bus::chain()`
**Resultado**: CSV ‚Üí Excel (DETTRA) ‚Üí Excel (PAGAPL) ‚Üí Excel (PAGPLA) ‚Üí SQL

### 5. PostgreSQL Transaction Abortion
**Problema**: "current transaction is aborted, commands ignored"
**Soluci√≥n**: Transacci√≥n individual por fila en `ResilientCsvImporter::processChunk()`

---

## üîß Configuraci√≥n Actual

### Horizon (config/horizon.php)
```php
'supervisor-default' => [
    'connection' => 'redis',
    'queue' => ['default'],
    'memory' => 2048,
    'timeout' => 14400,  // 4 horas
    'tries' => 1,
],
```

### Docker Compose (docker-compose.yml)
```yaml
poarl-horizon:
  command: php artisan horizon

poarl-worker:
  command: php artisan queue:work redis --queue=default --tries=1 --max-time=7200 --timeout=3600
```

### Base de Datos

**Tablas de Data Sources:**
- `data_source_bascar` (run_id, 58 columnas CSV)
- `data_source_baprpo` (run_id, data jsonb)
- `data_source_datpol` (run_id, data jsonb)
- `data_source_dettra` (run_id, data jsonb, sheet_name)
- `data_source_pagapl` (run_id, identificacion, periodo, valor, composite_key, data jsonb, sheet_name)
- `data_source_pagpla` (run_id, data jsonb, sheet_name)

**Tabla de Errores:**
- `csv_import_error_logs` (run_id, data_source_code, table_name, line_number, error_type, error_message)

---

## üìä Performance Actual

### Cargas Exitosas Registradas

**BASCAR (CSV - ResilientCsvImporter):**
- Registros: 255,178
- Duraci√≥n: ~23 minutos
- Errores: 0
- M√©todo: L√≠nea por l√≠nea con chunks de 10k

**DATPOL (CSV - ResilientCsvImporter):**
- Registros: 68,406
- Duraci√≥n: ~5 minutos
- Errores: 0 (antes: 19,375 errores por UTF-8)
- M√©todo: L√≠nea por l√≠nea con chunks de 10k + UTF-8 conversion

**BAPRPO (CSV - ResilientCsvImporter):**
- Registros: ~50k
- Duraci√≥n: ~2 minutos
- Errores: 0

**Excel Files (LoadExcelWithCopyJob - Go + PostgreSQL COPY):**
- DETTRA: ~202 MB, m√∫ltiples hojas
- PAGAPL: ~190 MB, m√∫ltiples hojas
- PAGPLA: ~289 MB, m√∫ltiples hojas
- Conversi√≥n: ~40 MB/s con Go streaming
- Carga: ~3s por 100MB con PostgreSQL COPY

---

## ‚úÖ Estado del Sistema

### Completado
- ‚úÖ Pipeline secuencial (CSV ‚Üí Excel ‚Üí SQL)
- ‚úÖ Eliminaci√≥n de duplicaci√≥n Jobs vs Processor
- ‚úÖ Validaci√≥n de datos cargados por jobs en processor
- ‚úÖ Idempotencia en todos los jobs de carga
- ‚úÖ UTF-8 conversion autom√°tica
- ‚úÖ Error logging granular
- ‚úÖ Timeouts adecuados (4 horas para CSV, 60 min para Excel)
- ‚úÖ PostgreSQL COPY funcionando con psql CLI
- ‚úÖ ResilientCsvImporter con manejo de errores individual

### Pendiente (TODOs en c√≥digo)
- ‚è≥ Step de depuraci√≥n de tablas (eliminar registros no necesarios)
- ‚è≥ Cleanup de datos despu√©s de procesamiento (comentado en BaseCollectionNoticeProcessor:109)
- ‚è≥ Nuevos cruces de datos (pendientes de definici√≥n de reglas)

---

## üöÄ Pr√≥ximos Pasos Sugeridos

1. **Verificar que el processor funciona correctamente**:
   - Ejecutar run completo y verificar que `ValidateDataIntegrityStep` pasa
   - Confirmar que los steps SQL ejecutan correctamente

2. **Implementar steps pendientes**:
   - Step de depuraci√≥n de tablas
   - Nuevos cruces de datos seg√∫n reglas de negocio

3. **Optimizaciones opcionales**:
   - Habilitar cleanup autom√°tico cuando el sistema est√© 100% estable
   - Considerar paralelizar jobs Excel si el sistema lo permite
   - Implementar retry strategy m√°s sofisticada para jobs de carga

---

## üìù Notas Importantes

- **NO usar `Bus::batch()` para estos jobs**: Usar `Bus::chain()` para garantizar orden secuencial
- **Jobs de carga son idempotentes**: Limpian tablas antes de insertar (basado en `run_id`)
- **Processor NO carga datos**: Solo valida y transforma (separaci√≥n de responsabilidades)
- **ResilientCsvImporter vs PostgreSQLCopyImporter**:
  - Resilient: Para CSVs problem√°ticos (encoding, errores), m√°s lento pero robusto
  - COPY: Para CSVs limpios, ultra r√°pido pero todo-o-nada
- **Docker rebuild necesario** si se cambia Dockerfile (ej: agregar postgresql-client)

---

**Autor**: Claude Code
**√öltima actualizaci√≥n**: 2025-10-05
**Estado**: Sistema operativo y optimizado
