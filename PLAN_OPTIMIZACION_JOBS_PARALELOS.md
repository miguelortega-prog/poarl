# Plan de Optimizaci√≥n - Jobs Paralelos para Carga de Data Sources
**Fecha:** 2025-10-02
**Estrategia:** Opci√≥n A - Jobs en Paralelo por Tipo de Archivo

---

## üìä Contexto y Problema

### An√°lisis de Performance Actual (Run #8)

| Paso | Duraci√≥n | % del Total | Tipo |
|------|----------|-------------|------|
| 1. Cargar archivos de insumos | 108.8s (1.8 min) | 11.2% | I/O Excel |
| 2. Validar integridad | 0.015s | 0.002% | SQL |
| 3. Filtrar BASCAR por periodo | 18.6s | 1.9% | SQL |
| 4. Generar composite keys BASCAR | 21.6s | 2.2% | SQL |
| **5. Cargar hoja PAGAPL** | **772.7s (12.9 min)** | **79.5%** üî¥ | I/O Excel |
| 6. Generar composite keys PAGAPL | 35.4s | 3.6% | SQL |
| **7. Cruzar BASCAR con PAGAPL** | **7.3s** ‚ö° | 0.7% | SQL optimizado |
| **Total:** | **964s (16 min)** | 100% | |

**Hallazgos clave:**
- üî¥ **91% del tiempo se va en cargar archivos Excel** (pasos 1 y 5)
- ‚úÖ **SQL es extremadamente r√°pido** (~8.5% del tiempo total)
- ‚úÖ **Cruce optimizado funciona perfecto** (7.3s con tabla temporal)
- ‚ùå **DETTRA (203 MB) nunca termin√≥** - el job crashe√≥

### Archivos de Insumos

**CSV (r√°pidos):**
- BASCAR - Base cartera
- BAPRPO - Base producci√≥n por p√≥liza
- DATPOL - Datos p√≥liza

**XLSX (lentos):**
- PAGAPL - Pagos aplicados (~191 MB) ‚Üí **772s (12.9 min)**
- DETTRA - Detalle trabajadores (~203 MB) ‚Üí **estimado 15+ min**
- PAGPLA - Pagos planilla (~289 MB) ‚Üí **estimado 20+ min**

---

## üéØ Soluci√≥n Propuesta: Jobs Paralelos

### Arquitectura Nueva

```
Usuario crea Run
    ‚Üì
[VALIDACI√ìN]
ValidateCollectionNoticeRunJob (actual, sin cambios)
    ‚Üì
    ‚îú‚îÄ‚Üí LoadCsvDataSourcesJob (paralelo, ~30s)
    ‚îÇ   ‚îú‚îÄ BASCAR (CSV)
    ‚îÇ   ‚îú‚îÄ BAPRPO (CSV)
    ‚îÇ   ‚îî‚îÄ DATPOL (CSV)
    ‚îÇ
    ‚îú‚îÄ‚Üí LoadPagaplDataSourceJob (paralelo, ~13min)
    ‚îÇ   ‚îî‚îÄ Solo hoja del periodo (ej: "2024-2025")
    ‚îÇ
    ‚îú‚îÄ‚Üí LoadDettraDataSourceJob (paralelo, ~15min)
    ‚îÇ   ‚îî‚îÄ Todas las hojas (ej: "Base")
    ‚îÇ
    ‚îî‚îÄ‚Üí LoadPagplaDataSourceJob (paralelo, ~20min)
        ‚îî‚îÄ Hoja del periodo
    ‚Üì
[Esperar que TODOS los jobs de carga completen]
    ‚Üì
[PROCESAMIENTO SQL PURO]
ProcessCollectionDataJob (~2-5min total)
    ‚îú‚îÄ Paso 1: Generar composite keys BASCAR (SQL)
    ‚îú‚îÄ Paso 2: Filtrar BASCAR por periodo (SQL)
    ‚îú‚îÄ Paso 3: Generar composite keys PAGAPL (SQL)
    ‚îú‚îÄ Paso 4: Filtrar PAGAPL por periodo (SQL)
    ‚îú‚îÄ Paso 5: CrossBascarWithPagaplStep (SQL + tabla temporal) ‚úÖ Ya optimizado
    ‚îú‚îÄ Paso 6: CrossBascarWithBaprpoStep (SQL)
    ‚îú‚îÄ Paso 7: CrossBascarWithPagplaStep (SQL)
    ‚îú‚îÄ Paso 8: CrossBascarWithDatpolStep (SQL)
    ‚îú‚îÄ Paso 9: CountDettraWorkersAndUpdateBascarStep (SQL)
    ‚îî‚îÄ Paso 10: GenerateOutputFilesStep
    ‚Üì
[COMPLETADO] ‚úÖ
```

### Tiempo Estimado

**Actual (secuencial):**
- Carga archivos: ~40-50 minutos
- Procesamiento SQL: ~5 minutos
- **Total: ~45-55 minutos**

**Nuevo (paralelo):**
- Carga en paralelo: ~20 minutos (el m√°s lento = PAGPLA)
- Procesamiento SQL: ~5 minutos
- **Total: ~25 minutos** ‚ö° **Reducci√≥n del 50%**

---

## üîß Implementaci√≥n

### Fase 1: Crear Jobs Especializados

#### 1.1 LoadCsvDataSourcesJob
**Archivo:** `app/Jobs/LoadCsvDataSourcesJob.php`

**Responsabilidades:**
- Cargar BASCAR (CSV)
- Cargar BAPRPO (CSV)
- Cargar DATPOL (CSV)
- **NO generar composite keys** (se har√° en ProcessCollectionDataJob)
- **NO filtrar por periodo** (se har√° en ProcessCollectionDataJob)
- Solo inserci√≥n masiva en tablas

**Tabla destino:**
- `data_source_bascar`
- `data_source_baprpo`
- `data_source_datpol`

**Par√°metros:**
- `$runId` - ID del run
- Cola: `csv-loading` (nueva cola)
- Timeout: 300s (5 min)
- Tries: 2

---

#### 1.2 LoadPagaplDataSourceJob
**Archivo:** `app/Jobs/LoadPagaplDataSourceJob.php`

**Responsabilidades:**
- Cargar **solo la hoja del periodo** de PAGAPL
- Ejemplo: Si periodo = "202508" (2025-08), cargar hoja "2024-2025"
- **NO generar composite keys** (se har√° en SQL)
- Solo inserci√≥n masiva

**Tabla destino:**
- `data_source_pagapl`

**Optimizaciones espec√≠ficas:**
- Chunk size: 10000 (en vez de 5000)
- Desactivar logs DEBUG
- Batch inserts m√°s grandes

**Par√°metros:**
- `$runId` - ID del run
- `$period` - Periodo (ej: "202508")
- Cola: `excel-loading` (nueva cola)
- Timeout: 1200s (20 min)
- Tries: 2

---

#### 1.3 LoadDettraDataSourceJob
**Archivo:** `app/Jobs/LoadDettraDataSourceJob.php`

**Responsabilidades:**
- Cargar **todas las hojas** de DETTRA
- Almacenar en columna JSONB `data`
- Solo inserci√≥n masiva

**Tabla destino:**
- `data_source_dettra`

**Optimizaciones espec√≠ficas:**
- Chunk size: 10000
- Desactivar logs DEBUG
- Batch inserts m√°s grandes

**Par√°metros:**
- `$runId` - ID del run
- Cola: `excel-loading` (nueva cola)
- Timeout: 1800s (30 min)
- Tries: 2

---

#### 1.4 LoadPagplaDataSourceJob
**Archivo:** `app/Jobs/LoadPagplaDataSourceJob.php`

**Responsabilidades:**
- Cargar **solo la hoja del periodo** de PAGPLA
- Almacenar en columna JSONB `data`
- Solo inserci√≥n masiva

**Tabla destino:**
- `data_source_pagpla`

**Optimizaciones espec√≠ficas:**
- Chunk size: 10000
- Desactivar logs DEBUG
- Batch inserts m√°s grandes

**Par√°metros:**
- `$runId` - ID del run
- `$period` - Periodo
- Cola: `excel-loading` (nueva cola)
- Timeout: 2400s (40 min - es el m√°s grande)
- Tries: 2

---

#### 1.5 ProcessCollectionDataJob
**Archivo:** `app/Jobs/ProcessCollectionDataJob.php`

**Responsabilidades:**
- **TODO en SQL puro, sin lectura de archivos**
- Ejecutar steps del processor **solo para operaciones SQL**

**Steps que ejecuta:**
1. ValidateDataIntegrityStep (verificar que todos los archivos cargaron)
2. FilterBascarByPeriodStep (SQL UPDATE)
3. GenerateBascarCompositeKeyStep (SQL UPDATE)
4. GeneratePagaplCompositeKeyStep (SQL UPDATE)
5. CrossBascarWithPagaplStep ‚úÖ (ya optimizado)
6. RemoveCrossedBascarRecordsStep (SQL DELETE)
7. CountDettraWorkersAndUpdateBascarStep (SQL UPDATE)
8. CrossBascarWithBaprpoStep (SQL - TODO)
9. CrossBascarWithPagplaStep (SQL - TODO)
10. CrossBascarWithDatpolStep (SQL - TODO)
11. GenerateOutputFilesStep (generar CSVs desde BD)

**Par√°metros:**
- `$runId` - ID del run
- Cola: `processing` (existente)
- Timeout: 1800s (30 min)
- Tries: 3

---

### Fase 2: Dispatcher con Laravel Bus

**Archivo a modificar:** `app/Jobs/ProcessCollectionRunValidation.php`

**Cambio en l√≠nea 96-102:**

```php
// ANTES (c√≥digo actual):
if ($validationSuccess) {
    Log::info('Disparando job de procesamiento de datos', [
        'run_id' => $run->id,
    ]);

    ProcessCollectionNoticeRunData::dispatch($run->id);
}

// DESPU√âS (nuevo c√≥digo):
if ($validationSuccess) {
    Log::info('Disparando jobs de carga de data sources en paralelo', [
        'run_id' => $run->id,
    ]);

    // Despachar jobs de carga en paralelo
    Bus::batch([
        new LoadCsvDataSourcesJob($run->id),
        new LoadPagaplDataSourceJob($run->id, $run->period),
        new LoadDettraDataSourceJob($run->id),
        new LoadPagplaDataSourceJob($run->id, $run->period),
    ])
    ->name("Carga de Data Sources - Run #{$run->id}")
    ->then(function (Batch $batch) use ($run) {
        // Cuando TODOS los jobs de carga completen, disparar procesamiento
        Log::info('Todos los archivos cargados, iniciando procesamiento SQL', [
            'run_id' => $run->id,
        ]);

        ProcessCollectionDataJob::dispatch($run->id);
    })
    ->catch(function (Batch $batch, Throwable $e) use ($run) {
        // Si alg√∫n job de carga falla
        Log::error('Error en carga de data sources', [
            'run_id' => $run->id,
            'error' => $e->getMessage(),
        ]);

        $run->update([
            'status' => 'failed',
            'failed_at' => now(),
            'errors' => [
                'message' => 'Error durante la carga de archivos',
                'details' => $e->getMessage(),
            ],
        ]);
    })
    ->allowFailures(false) // Si uno falla, detener todo
    ->onQueue('validation')
    ->dispatch();
}
```

---

### Fase 3: Configurar Colas en Horizon

**Archivo a modificar:** `config/horizon.php`

**Agregar nuevas colas:**

```php
'environments' => [
    'production' => [
        'supervisor-validation' => [
            'connection' => 'redis',
            'queue' => ['validation'],
            'balance' => 'auto',
            'processes' => 1,
            'tries' => 3,
        ],

        // NUEVA: Cola para carga de CSV (r√°pida)
        'supervisor-csv-loading' => [
            'connection' => 'redis',
            'queue' => ['csv-loading'],
            'balance' => 'auto',
            'processes' => 1,
            'tries' => 2,
            'timeout' => 300,
        ],

        // NUEVA: Cola para carga de Excel (lenta, m√∫ltiples workers)
        'supervisor-excel-loading' => [
            'connection' => 'redis',
            'queue' => ['excel-loading'],
            'balance' => 'auto',
            'processes' => 3, // 3 workers para procesar 3 Excel en paralelo
            'tries' => 2,
            'timeout' => 2400, // 40 min
        ],

        'supervisor-processing' => [
            'connection' => 'redis',
            'queue' => ['processing'],
            'balance' => 'auto',
            'processes' => 1,
            'tries' => 3,
            'timeout' => 1800,
        ],
    ],
],
```

---

### Fase 4: Refactorizar Steps Existentes

#### Steps que se ELIMINAN del Processor:
- ‚ùå `LoadDataSourceFilesStep` ‚Üí Movido a jobs de carga
- ‚ùå `LoadPagaplSheetByPeriodStep` ‚Üí Movido a LoadPagaplDataSourceJob

#### Steps que se MANTIENEN (operaciones SQL):
- ‚úÖ `ValidateDataIntegrityStep` ‚Üí Verifica que datos existan en BD
- ‚úÖ `FilterBascarByPeriodStep` ‚Üí SQL UPDATE
- ‚úÖ `GenerateBascarCompositeKeyStep` ‚Üí SQL UPDATE
- ‚úÖ `GeneratePagaplCompositeKeyStep` ‚Üí SQL UPDATE
- ‚úÖ `CrossBascarWithPagaplStep` ‚Üí SQL (ya optimizado)
- ‚úÖ `RemoveCrossedBascarRecordsStep` ‚Üí SQL DELETE (necesita fix)
- ‚úÖ `LoadDettraAllSheetsStep` ‚Üí Movido a LoadDettraDataSourceJob
- ‚úÖ `CountDettraWorkersAndUpdateBascarStep` ‚Üí SQL UPDATE

#### Steps NUEVOS a implementar:
- üÜï `CrossBascarWithBaprpoStep`
- üÜï `CrossBascarWithPagplaStep`
- üÜï `CrossBascarWithDatpolStep`
- üÜï `GenerateOutputFilesStep`

---

## üìù Optimizaciones Adicionales

### 1. Composite Keys en SQL (m√°s r√°pido)

**Antes (durante carga):**
```php
// Concatenar en PHP durante inserci√≥n
$data['composite_key'] = $row['NUM_TOMADOR'] . '-' . $period;
```

**Despu√©s (en SQL despu√©s de carga completa):**
```sql
-- Mucho m√°s r√°pido con UPDATE masivo
UPDATE data_source_bascar
SET composite_key = num_tomador || '-' || periodo
WHERE run_id = ? AND composite_key IS NULL;

-- Crear √≠ndice concurrentemente (sin bloquear tabla)
CREATE INDEX CONCURRENTLY idx_bascar_composite_run
ON data_source_bascar (composite_key, run_id)
WHERE run_id = ?;
```

### 2. Desactivar Logs DEBUG durante carga masiva

```php
// En jobs de carga
public function handle()
{
    // Guardar nivel actual
    $originalLevel = config('logging.level');

    // Cambiar a INFO (sin DEBUG)
    config(['logging.level' => 'info']);

    try {
        // Carga masiva...
    } finally {
        // Restaurar nivel original
        config(['logging.level' => $originalLevel]);
    }
}
```

### 3. Chunk Size m√°s grande para Excel

```php
// ANTES
$chunkSize = 5000;

// DESPU√âS (para archivos grandes)
$chunkSize = 10000; // o 15000
```

---

## üß™ Plan de Pruebas

### Test 1: Jobs de Carga en Paralelo
1. Crear Run #9
2. Verificar que se disparan 4 jobs simult√°neamente
3. Monitorear CPU/RAM de PostgreSQL
4. Verificar que todos completan exitosamente
5. Medir tiempo total de carga

**Expectativa:** ~20 minutos (vs ~40-50 actual)

### Test 2: Procesamiento SQL
1. Verificar que `ProcessCollectionDataJob` se dispara autom√°ticamente
2. Verificar que todos los steps SQL ejecutan correctamente
3. Verificar que no hay errores de composite_key
4. Medir tiempo de procesamiento SQL

**Expectativa:** ~5 minutos

### Test 3: Run Completo End-to-End
1. Crear Run #10
2. Medir tiempo total desde creaci√≥n hasta completado
3. Verificar archivos de salida generados
4. Validar datos en BD

**Expectativa:** ~25 minutos total

---

## üìã Checklist de Implementaci√≥n

### Fase 1: Crear Jobs
- [ ] `LoadCsvDataSourcesJob.php`
- [ ] `LoadPagaplDataSourceJob.php`
- [ ] `LoadDettraDataSourceJob.php`
- [ ] `LoadPagplaDataSourceJob.php`
- [ ] `ProcessCollectionDataJob.php`

### Fase 2: Configuraci√≥n
- [ ] Modificar `ProcessCollectionRunValidation.php` (dispatcher)
- [ ] Configurar colas en `config/horizon.php`
- [ ] Reiniciar Horizon

### Fase 3: Refactorizar Steps
- [ ] Actualizar `ConstitucionMoraAportantesProcessor.php`
- [ ] Actualizar `ValidateDataIntegrityStep` para verificar BD
- [ ] Optimizar `GenerateBascarCompositeKeyStep` (SQL puro)
- [ ] Optimizar `GeneratePagaplCompositeKeyStep` (SQL puro)
- [ ] Fix `RemoveCrossedBascarRecordsStep` (actualizar shouldExecute)

### Fase 4: Steps Nuevos
- [ ] Implementar `CrossBascarWithBaprpoStep`
- [ ] Implementar `CrossBascarWithPagplaStep`
- [ ] Implementar `CrossBascarWithDatpolStep`
- [ ] Implementar `GenerateOutputFilesStep`

### Fase 5: Testing
- [ ] Test unitario de cada job
- [ ] Test de integraci√≥n del batch
- [ ] Test end-to-end con Run completo
- [ ] Monitoreo de recursos (CPU/RAM)

---

## üéØ M√©tricas de √âxito

| M√©trica | Antes | Meta | Medici√≥n |
|---------|-------|------|----------|
| Tiempo de carga | ~40-50 min | ~20 min | ‚è±Ô∏è Tiempo hasta ProcessCollectionDataJob |
| Tiempo total | ~50-60 min | ~25 min | ‚è±Ô∏è created_at ‚Üí completed_at |
| CPU PostgreSQL | 99% (bloqueado) | <60% | üìä docker stats |
| Errores de memoria | S√≠ | No | ‚úÖ Sin crashes |
| Reintentos exitosos | N/A | >80% | üìä failed_jobs table |

---

## üìö Referencias

**Archivos clave:**
- `app/Jobs/ProcessCollectionNoticeRunData.php` (actual, a reemplazar)
- `app/Jobs/ProcessCollectionRunValidation.php` (modificar dispatcher)
- `app/UseCases/Recaudo/Comunicados/Processors/ConstitucionMoraAportantesProcessor.php`
- `app/UseCases/Recaudo/Comunicados/Steps/CrossBascarWithPagaplStep.php` ‚úÖ (ya optimizado)
- `config/horizon.php` (agregar colas)

**Optimizaciones aplicadas:**
- ‚úÖ CrossBascarWithPagaplStep optimizado con tabla temporal (7.3s)
- ‚úÖ √çndices optimizados aplicados (`idx_bascar_composite_run`, `idx_pagapl_composite_run`)
- ‚úÖ Migraci√≥n de columnas trabajadores aplicada
- ‚úÖ Logs DEBUG eliminados de ServiceProvider y CreateRunModal

**Pendiente:**
- üîÑ Implementar jobs paralelos (este documento)
- üîÑ Implementar steps faltantes (BAPRPO, PAGPLA, DATPOL)
- üîÑ Implementar generaci√≥n de archivos de salida

---

**Estado:** ‚úÖ IMPLEMENTACI√ìN COMPLETA
**Pr√≥ximo paso:** Crear Run de prueba para validar el sistema
**√öltima actualizaci√≥n:** 2025-10-02 23:50 UTC

---

## ‚úÖ Implementaci√≥n Completada

### Jobs Creados:
1. ‚úÖ `app/Jobs/LoadCsvDataSourcesJob.php` - Carga CSV en paralelo
2. ‚úÖ `app/Jobs/LoadPagaplDataSourceJob.php` - Carga PAGAPL por periodo
3. ‚úÖ `app/Jobs/LoadDettraDataSourceJob.php` - Carga todas las hojas DETTRA
4. ‚úÖ `app/Jobs/LoadPagplaDataSourceJob.php` - Carga PAGPLA por periodo
5. ‚úÖ `app/Jobs/ProcessCollectionDataJob.php` - Procesamiento SQL puro

### Configuraci√≥n Aplicada:
- ‚úÖ Modificado `ProcessCollectionRunValidation.php` con `Bus::batch()`
- ‚úÖ Configuradas nuevas colas en `config/horizon.php`:
  - `csv-loading` (1 worker, 512MB, 5min timeout)
  - `excel-loading` (3 workers, 2GB, 40min timeout)
  - `processing` (2 workers, 2GB, 30min timeout)
- ‚úÖ Refactorizado `ConstitucionMoraAportantesProcessor` (solo SQL)
- ‚úÖ Horizon reiniciado exitosamente

### Arquitectura Implementada:
```
Usuario crea Run
    ‚Üì
ValidateCollectionNoticeRunJob
    ‚Üì
Bus::batch() - 4 jobs en paralelo:
    ‚îú‚îÄ‚Üí LoadCsvDataSourcesJob (BASCAR, BAPRPO, DATPOL)
    ‚îú‚îÄ‚Üí LoadPagaplDataSourceJob (hoja del periodo)
    ‚îú‚îÄ‚Üí LoadDettraDataSourceJob (todas las hojas)
    ‚îî‚îÄ‚Üí LoadPagplaDataSourceJob (hoja del periodo)
    ‚Üì
then() ‚Üí ProcessCollectionDataJob (SQL puro)
    ‚Üì
‚úÖ COMPLETADO
```
