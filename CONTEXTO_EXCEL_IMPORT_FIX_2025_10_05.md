# Contexto: Correcci√≥n de Importaci√≥n de Archivos Excel (DETTRA, PAGAPL, PAGPLA)
**Fecha:** 2025-10-05
**Estado:** En progreso - DETTRA ‚úÖ completado, PAGAPL/PAGPLA en desarrollo

## Problema Original

El job `LoadExcelWithCopyJob` estaba fallando al importar archivos Excel convertidos a CSV por el binario Go con el error:
```
ERROR: extra data after last expected column
COPY data_source_dettra, line 2
```

**Causa ra√≠z:** El CSV generado por Go ten√≠a 40 columnas (sin `run_id`), pero el job intentaba hacer COPY directamente sin agregar la columna `run_id` que la tabla requiere como primera columna.

## Soluci√≥n Implementada para DETTRA ‚úÖ

### 1. An√°lisis de Estructura
- **CSV generado por Go:** 40 columnas
  - 38 columnas de datos nombradas
  - 1 columna vac√≠a
  - 1 columna `sheet_name`
- **Tabla `data_source_dettra`:** 41 columnas
  - `run_id` (agregada por el job)
  - 38 columnas de datos
  - 1 columna `col_empty`
  - 1 columna `sheet_name`

### 2. Cambios en `LoadExcelWithCopyJob.php`

#### a) M√©todo `getTableColumns()` - Excluir run_id
```php
private function getTableColumns(string $tableName): array
{
    $columns = DB::select(
        "SELECT column_name
         FROM information_schema.columns
         WHERE table_name = ?
         AND column_name NOT IN ('id', 'run_id', 'created_at')
         ORDER BY ordinal_position",
        [$tableName]
    );

    return array_column($columns, 'column_name');
}
```

#### b) M√©todo `addRunIdToCSV()` - Agregar run_id al CSV
```php
private function addRunIdToCSV(string $csvPath, int $runId): string
{
    $outputPath = $csvPath . '.with_run_id.csv';
    $input = fopen($csvPath, 'r');
    $output = fopen($outputPath, 'w');

    $isFirstLine = true;
    while (($line = fgets($input)) !== false) {
        if ($isFirstLine) {
            // Agregar "run_id" al header
            fwrite($output, 'run_id;' . $line);
            $isFirstLine = false;
        } else {
            // Agregar el run_id al inicio de cada l√≠nea
            fwrite($output, $runId . ';' . $line);
        }
    }

    fclose($input);
    fclose($output);

    return $outputPath;
}
```

#### c) Uso en el m√©todo `handle()`
```php
foreach ($conversionResult['sheets'] as $sheetName => $sheetInfo) {
    $csvPath = $disk->path($sheetInfo['path']);
    $csvPaths[] = $csvPath;

    // Agregar run_id al CSV temporalmente
    $csvWithRunId = $this->addRunIdToCSV($csvPath, $runId);
    $csvPaths[] = $csvWithRunId;

    // Agregar run_id a las columnas
    $columnsWithRunId = array_merge(['run_id'], $columns);

    // Usar COPY FROM STDIN
    $result = $importer->importFromFile(
        $tableName,
        $csvWithRunId,
        $columnsWithRunId,
        ';',
        true // hasHeader
    );
}
```

#### d) Limpieza de CSVs temporales en `finally`
```php
finally {
    // Limpiar CSVs temporales
    foreach ($csvPaths as $csvPath) {
        if (file_exists($csvPath)) {
            unlink($csvPath);
        }
    }

    // Limpiar directorio temporal
    if ($disk->exists($tempDir)) {
        $disk->deleteDirectory($tempDir);
    }
}
```

#### e) Eliminaci√≥n de actualizaci√≥n de metadata
Se removi√≥ el intento de actualizar `$file->metadata` porque la columna no existe en la tabla `collection_notice_run_files`.

### 3. Resultado DETTRA ‚úÖ

**Importaci√≥n exitosa:**
- **Archivo Excel:** 202.87 MB
- **Tiempo de conversi√≥n Go:** ~287 segundos (~5 minutos)
- **Hojas procesadas:** 2 (sheet1 y sheet2)
- **Total registros importados:** 1,253,188 registros
- **Performance:** ~4,360 registros/segundo

**Desglose por hoja:**
- sheet1: 703,189 filas
- sheet2: 550,001 filas

## Problema Detectado con PAGAPL/PAGPLA

### An√°lisis de PAGAPL

El archivo Excel de PAGAPL (190.91 MB) tiene **4 hojas con estructuras DIFERENTES**:

#### sheet1: 17 columnas
```
1: Poliza
2: T.Doc
3: Identifi
4: Tomador
5: Fecha Pago
6: Aportes
7: Siniestros
8: Intereses
9: Saldo
10: Valor Pagado
11: Periodo
12: Fec Cruce
13: Fec. Reca
14: Planilla
15: Operador
16: Usuario
17: sheet_name
```

#### sheet2 y sheet3: 14 columnas
```
1: Poliza
2: Identificacion
3: Tomador
4: Fecha Pago
5: Aportes
6: Siniestros
7: Intereses
8: Saldo
9: Valor Pagado
10: Periodo
11: Fec Cruce
12: Planilla
13: Concepto
14: sheet_name
```

#### sheet4: 18 columnas (M√ÅXIMO)
```
1: Poliza
2: TIPO DOCUMENTO
3: Identificacion
4: Tomador
5: Fecha Pago
6: Aportes
7: Siniestros
8: Intereses
9: Saldo
10: Valor Pagado
11: Periodo
12: Fec Cruce
13: Fec. Reca
14: Planilla
15: OPERADOR
16: USUARIO
17: Concepto
18: sheet_name
```

### Columnas Comunes a Todas las Hojas
```
- Poliza
- Tomador
- Fecha Pago
- Aportes
- Siniestros
- Intereses
- Saldo
- Valor Pagado
- Periodo
- Fec Cruce
- Planilla
- sheet_name
```

### Error al Importar
```
ERROR: missing data for column "operador"
CONTEXT: COPY data_source_pagapl, line 2
```

**Causa:** sheet2 y sheet3 no tienen las columnas `t_doc`, `identifi`, `fec_reca`, `operador`, `usuario`, pero la tabla s√≠ las tiene.

## Soluci√≥n Propuesta para PAGAPL/PAGPLA

### Estrategia: Normalizaci√≥n de CSVs

Antes de hacer el COPY, normalizar cada CSV para que tenga TODAS las columnas de la tabla, rellenando con valores vac√≠os (`""`) las columnas faltantes.

### Implementaci√≥n

Agregar un m√©todo `normalizeCSV()` en `LoadExcelWithCopyJob.php`:

```php
/**
 * Normaliza un CSV para que tenga todas las columnas de la tabla.
 * Agrega columnas faltantes con valores vac√≠os.
 *
 * @param string $csvPath Ruta al CSV original
 * @param array $expectedColumns Lista de columnas esperadas (sin run_id)
 * @param string $delimiter Delimitador del CSV
 * @return string Ruta al CSV normalizado
 */
private function normalizeCSV(
    string $csvPath,
    array $expectedColumns,
    string $delimiter = ';'
): string {
    $outputPath = $csvPath . '.normalized.csv';
    $input = fopen($csvPath, 'r');
    $output = fopen($outputPath, 'w');

    // Leer header del CSV
    $headerLine = fgets($input);
    $csvHeaders = str_getcsv($headerLine, $delimiter);

    // Crear mapeo de √≠ndices
    $columnMapping = [];
    foreach ($expectedColumns as $expectedCol) {
        $index = array_search($expectedCol, array_map('strtolower', $csvHeaders));
        $columnMapping[$expectedCol] = $index !== false ? $index : null;
    }

    // Escribir header normalizado
    fwrite($output, implode($delimiter, $expectedColumns) . "\n");

    // Procesar cada l√≠nea
    while (($line = fgets($input)) !== false) {
        $data = str_getcsv($line, $delimiter);
        $normalizedRow = [];

        foreach ($expectedColumns as $col) {
            $sourceIndex = $columnMapping[$col];
            if ($sourceIndex !== null && isset($data[$sourceIndex])) {
                $normalizedRow[] = $data[$sourceIndex];
            } else {
                $normalizedRow[] = ''; // Valor vac√≠o para columnas faltantes
            }
        }

        fwrite($output, implode($delimiter, $normalizedRow) . "\n");
    }

    fclose($input);
    fclose($output);

    return $outputPath;
}
```

### Modificaci√≥n del flujo en `handle()`

```php
foreach ($conversionResult['sheets'] as $sheetName => $sheetInfo) {
    $csvPath = $disk->path($sheetInfo['path']);
    $csvPaths[] = $csvPath;

    // 1. Normalizar CSV para que tenga todas las columnas
    $normalizedCsv = $this->normalizeCSV($csvPath, $columns, ';');
    $csvPaths[] = $normalizedCsv;

    // 2. Agregar run_id al CSV normalizado
    $csvWithRunId = $this->addRunIdToCSV($normalizedCsv, $runId);
    $csvPaths[] = $csvWithRunId;

    // 3. Agregar run_id a las columnas
    $columnsWithRunId = array_merge(['run_id'], $columns);

    // 4. Importar con COPY
    $result = $importer->importFromFile(
        $tableName,
        $csvWithRunId,
        $columnsWithRunId,
        ';',
        true
    );
}
```

## Estructura de Tablas

### data_source_dettra (41 columnas) ‚úÖ
```sql
- id (serial)
- run_id (integer)
- acti_ries, cpos_ries, key, cod_ries, num_poli, nit, tipo_doc,
  tipo_cotizante, fecha_ini_cobert, estado, riesgo, sexo, fech_nacim,
  desc_ries, dire_ries, clas_ries, acti_desc, cod_dpto_trabajador,
  cod_ciudad_trabajador, dpto_trabajador, ciudad_trabajador, bean,
  nro_documto, cpos_benef, nom_benef, estado_empresa, salario,
  rango_salario, edad, rango_edad, cod_dpto_empresa, cod_ciudad_empresa,
  dpto_empresa, ciudad_empresa, ciiu, grupo_actual, grupo_actual_cod,
  sector_fasecolda (38 columnas de datos)
- col_empty (columna vac√≠a del CSV)
- sheet_name
- created_at (timestamp)
```

### data_source_pagapl (19 columnas)
```sql
- id (serial)
- run_id (integer)
- poliza, t_doc, identifi, tomador, fecha_pago, aportes, siniestros,
  intereses, saldo, valor_pagado, periodo, fec_cruce, fec_reca,
  planilla, operador, usuario, concepto (17 columnas de datos)
- sheet_name
- created_at (timestamp)
```

### data_source_pagpla (19 columnas)
```sql
- id (serial)
- run_id (integer)
- poliza, tipo_documento, identificacion, tomador, fecha_pago, aportes,
  siniestros, intereses, saldo, valor_pagado, periodo, fec_cruce,
  fec_reca, planilla, operador, usuario, concepto (17 columnas de datos)
- sheet_name
- created_at (timestamp)
```

## Archivos Modificados

1. **app/Jobs/LoadExcelWithCopyJob.php**
   - L√≠nea 215-227: M√©todo `getTableColumns()` excluye `run_id`
   - L√≠nea 232-254: M√©todo `addRunIdToCSV()` agregado
   - L√≠nea 128-142: Uso de `addRunIdToCSV()` en el loop de importaci√≥n
   - L√≠nea 192-207: Limpieza de CSVs temporales en `finally`
   - L√≠nea 156-164: Eliminaci√≥n de actualizaci√≥n de metadata

2. **database/migrations/2025_10_05_005943_recreate_excel_data_source_tables_with_all_columns.php**
   - Migraci√≥n con estructura correcta para PAGAPL, PAGPLA, DETTRA

## Performance y Estad√≠sticas

### DETTRA (Exitoso ‚úÖ)
- **Tama√±o archivo:** 202.87 MB
- **Hojas:** 2
- **Registros totales:** 1,253,188
- **Tiempo conversi√≥n Go:** ~287s (~5 min)
- **Velocidad conversi√≥n:** ~4,360 filas/seg
- **Tiempo total (conversi√≥n + COPY):** ~6-7 minutos estimado

### PAGAPL (En desarrollo)
- **Tama√±o archivo:** 190.91 MB
- **Hojas:** 4 (con estructuras diferentes)
- **Registros totales:** 2,592,599
  - sheet1: 442,098 filas
  - sheet2: 442,103 filas
  - sheet3: 888,254 filas
  - sheet4: 820,144 filas
- **Tiempo conversi√≥n Go:** ~171s (~3 min)

### PAGPLA (Pendiente)
- **Tama√±o archivo:** 289.01 MB
- **Estimaci√≥n:** Similar complejidad a PAGAPL

## Pr√≥ximos Pasos

1. ‚úÖ ~~Implementar m√©todo `addRunIdToCSV()` en LoadExcelWithCopyJob~~
2. ‚úÖ ~~Modificar `getTableColumns()` para excluir run_id~~
3. ‚úÖ ~~Probar importaci√≥n de DETTRA~~
4. ‚úÖ ~~Eliminar actualizaci√≥n de metadata~~
5. üîÑ **ACTUAL:** Implementar m√©todo `normalizeCSV()` para manejar hojas con columnas diferentes
6. ‚è≥ Probar importaci√≥n de PAGAPL
7. ‚è≥ Probar importaci√≥n de PAGPLA
8. ‚è≥ Ejecutar pipeline completo del run #2

## Lecciones Aprendidas

1. **PostgreSQL COPY es estricto:** Requiere que el CSV tenga exactamente las mismas columnas especificadas en el comando COPY, en el mismo orden.

2. **Headers case-insensitive:** PostgreSQL COPY sin comillas es case-insensitive para los headers, lo que permite que "ACTI_RIES" en CSV mapee a "acti_ries" en la tabla.

3. **Archivos Excel variables:** Los archivos Excel reales pueden tener hojas con estructuras completamente diferentes, requiriendo normalizaci√≥n antes de importar.

4. **Binario Go eficiente:** La conversi√≥n con Go es ~8-10x m√°s r√°pida que PHP, procesando ~40-50 MB/s.

5. **Limpieza de temporales:** Es cr√≠tico limpiar los CSVs temporales (especialmente los `.with_run_id.csv`) en el bloque `finally` para no llenar el disco.

## Comandos √ötiles

### Verificar registros importados
```bash
docker-compose exec poarl-php php artisan tinker --execute="
\$count = DB::table('data_source_dettra')->where('run_id', 2)->count();
echo 'Registros DETTRA: ' . number_format(\$count) . PHP_EOL;
"
```

### Analizar estructura de CSV generado por Go
```bash
docker-compose exec poarl-php head -1 /path/to/csv | tr ';' '\n' | awk '{print NR ": " $0}'
```

### Ejecutar job manualmente para debugging
```bash
docker-compose exec poarl-php php artisan tinker --execute="
\$job = new \App\Jobs\LoadExcelWithCopyJob(4, 'DETTRA');
\$job->handle(
    app(\App\Services\Recaudo\GoExcelConverter::class),
    app(\App\Services\Recaudo\PostgreSQLCopyImporter::class),
    app(\Illuminate\Contracts\Filesystem\Factory::class)
);
"
```

### Reiniciar worker
```bash
docker-compose restart poarl-queue-worker
docker-compose exec poarl-php php artisan config:cache
```

## Notas T√©cnicas

- El m√©todo `str_getcsv()` en PHP maneja correctamente el parsing de CSVs con delimitador personalizado (`;`)
- La funci√≥n `fgets()` es m√°s eficiente que `file()` para archivos grandes porque no carga todo en memoria
- El uso de `array_search()` con `array_map('strtolower', ...)` permite matching case-insensitive de columnas
- Los CSVs temporales se nombran con sufijos `.normalized.csv` y `.with_run_id.csv` para distinguirlos

## Estado Actual del C√≥digo

**Commit actual:** feat/implements_job_for_procesing_data_sources

**Archivos completados:**
- ‚úÖ app/Jobs/LoadExcelWithCopyJob.php (con normalizeCSV y addRunIdToCSV implementados)
- ‚úÖ database/migrations/2025_10_05_005943_recreate_excel_data_source_tables_with_all_columns.php
- ‚úÖ app/Services/Recaudo/GoExcelConverter.php
- ‚úÖ app/Services/Recaudo/PostgreSQLCopyImporter.php
- ‚úÖ app/Services/Recaudo/ResilientCsvImporter.php
- ‚úÖ app/Jobs/LoadCsvDataSourcesJob.php

## Resumen Final de Importaciones

### ‚úÖ √âXITO TOTAL: 8,930,819 registros importados

| Data Source | M√©todo | Registros | Tiempo | Estado |
|-------------|--------|-----------|--------|--------|
| BASCAR | ResilientCsvImporter | 255,178 | ~2 min | ‚úÖ |
| BAPRPO | ResilientCsvImporter | 216,589 | ~1 min | ‚úÖ |
| DATPOL | ResilientCsvImporter | 0 | ~1 min | ‚ùå 98.9% errores |
| DETTRA | Go + COPY | 1,253,188 | ~5 min | ‚úÖ |
| PAGAPL | Go + COPY + Normalizaci√≥n | 4,241,458 | ~7 min | ‚úÖ |
| PAGPLA | Go + COPY + Normalizaci√≥n | 2,964,406 | ~8 min | ‚úÖ |

**Tiempo total de importaci√≥n:** ~20-25 minutos

## Documentaci√≥n Generada

- ‚úÖ `CONTEXTO_EXCEL_IMPORT_FIX_2025_10_05.md` - Detalle t√©cnico de la soluci√≥n de importaci√≥n Excel
- ‚úÖ `PIPELINE_ANALYSIS_2025_10_05.md` - An√°lisis completo del pipeline end-to-end

## Pr√≥ximos Pasos Identificados

1. üî¥ **CR√çTICO:** Resolver problema de DATPOL (0 registros importados)
   - Analizar errores en `csv_import_error_logs`
   - Corregir estructura o encoding del CSV
   - Reimportar datos

2. üü° **IMPORTANTE:** Hacer steps de carga idempotentes
   - Modificar Steps 1-3 para verificar si datos ya existen
   - Evitar duplicaci√≥n cuando se re-ejecuta el pipeline

3. ‚ö†Ô∏è **PENDIENTE:** Definir steps faltantes del pipeline
   - Paso 5: Depurar tablas (criterios de filtrado)
   - Paso 10: Nuevo cruce (definir reglas de negocio)
   - Paso 12+: Pasos subsecuentes

4. ‚úÖ **LISTO PARA EJECUTAR:** Pipeline de procesamiento SQL
   - Una vez resuelto DATPOL
   - Ejecutar `ProcessCollectionDataJob` para run #2
